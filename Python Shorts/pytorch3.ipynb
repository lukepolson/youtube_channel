{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b766e8a3-ea3c-4e69-8e6a-0cfd9ab9d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afd9f8-85fe-40f7-8913-5d5933dd081b",
   "metadata": {},
   "source": [
    "# Machine Learning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ace27-b0b5-4a50-80cd-b52705d62ae6",
   "metadata": {},
   "source": [
    "Suppose one has data that consists of an independent vector and a dependent vector $x_i$ and $y_i$ ($i$ is the ith value in the data set). For example:\n",
    "\n",
    "* $x_i$ is the height of the $i$th person, and $y_i$ is their weight (predict weight using height)\n",
    "* $x_i$ is a picture of a handwritten digit, and $y_i$ is the digit itself (predict numbers from hand written numbers)\n",
    "* $x_i$ is a CT scan of a patient, and $y_i$ are the corresponding pixels corresponding to tumours (my research)\n",
    "\n",
    "The goal of a neural network is as follows. Define a function $f$ that depends on parameters $a$ that makes predictions\n",
    "\n",
    "$$\\hat{y_i} =f(x_i;a)$$\n",
    "\n",
    "One wants to make $\\hat{y_i}$ (the predictions) and $y_i$ (the true values) as close as possible by modifying the values of $a$. What does as close as possible mean? This depends on the task. In general, one defines a similarity function (or **Loss** function) $L(y,\\hat{y})$. The more similar all the $y_i$s and $\\hat{y_i}$s are, the smaller $L$ should be. For example 1 above, this could be as simple as \n",
    "\n",
    "$$L(y,\\hat{y}) = \\sum_i(y_i-\\hat{y_i})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169d253-e893-47a3-9567-284345c667ce",
   "metadata": {},
   "source": [
    "# Last Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c646f3c-2d0c-4c1e-9e94-0d097c7c666d",
   "metadata": {},
   "source": [
    "Given 4 data points $(x_i, y_i)$ and wanted to find a function $f$ such that $f(x_i)=y_i$. Since each $x_i$ was a vector of length $2$, we chose the function\n",
    "\n",
    "$$f(x) = A_2 A_1 x$$\n",
    "\n",
    "where $A_1$ is a $8 \\times 2$ matrix and $A_2$ is a $1 \\times 8$ matrix. This means there were $16+8$ free parameters. This simple function did not do a very good job of making $f(x_i)=y_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148c9079-5b07-49d4-af8e-6ecc0d997b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "\n",
    "class MyNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,8,bias=False)\n",
    "        self.Matrix2 = nn.Linear(8,1,bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.Matrix1(x)\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()\n",
    "    \n",
    "f = MyNeuralNet()\n",
    "opt = SGD(f.parameters(), lr=0.001)\n",
    "L = nn.MSELoss()\n",
    "\n",
    "# Train model\n",
    "losses = []\n",
    "for _ in range(50):\n",
    "    opt.zero_grad() # flush previous epoch's gradient\n",
    "    loss_value = L(f(x), y) #compute loss\n",
    "    loss_value.backward() # compute gradient\n",
    "    opt.step() # Perform iteration using gradient above\n",
    "    losses.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feac8d75-1cf1-4b20-b7f9-75a19e27fb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64ea740-d0bb-4144-a8f4-51f17fa85356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2595, 2.8164, 1.3443, 4.9042], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453716f-36c9-4104-bdb8-de852cc4018b",
   "metadata": {},
   "source": [
    "# This Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6cc50c-818d-4845-a780-1e9d29138cd9",
   "metadata": {},
   "source": [
    "As it turns out, our previous model was not very good at all. In fact, although there were 24 parameters in the two matrices, there was technically only **two** independent parameters. This is because\n",
    "\n",
    "$$A_2 A_1 = B$$\n",
    "\n",
    "where $B$ is a $2 \\times 1$ matrix. So really our function was $f(x) = Bx$\n",
    "\n",
    "## How can we use this simplicity of linear algebra but have advanced models?\n",
    "\n",
    "**The Crux of Machine Learning**: This lies in so-called activation functions, which add ever-so-slight non-linearities to a sequence of matrix transformations. Instead of the transformation\n",
    "\n",
    "$$\\text{Old Model}: \\hspace{5mm} f(x) = A_2 A_1 x$$\n",
    "\n",
    "consider instead\n",
    "\n",
    "$$\\text{New Model}: \\hspace{5mm} f_2(x) = A_2 R(A_1 x)$$\n",
    "\n",
    "where $R$ is an element-wise operator defined by\n",
    "\n",
    "$$R(x) = \\begin{cases}x & x>0 \\\\ 0&  x \\leq 0 \\end{cases}$$\n",
    "\n",
    "So $R$ is the identity function if $x>0$ but sets values equal to zero if $x$ is less than zero. This is **so-close** to being a linear operator, but it is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88da39d3-262b-4f6f-8df3-db4e774ccf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  6,  2, -1,  6,  2,  5],\n",
       "        [ 1,  6,  2, -6,  5, -3,  5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[4,6,2,-1,6,2,5],[1,6,2,-6,5,-3,5]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4f1ea9-7122-4a84-b704-a11a4ef8b16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 6, 2, 0, 6, 2, 5],\n",
       "        [1, 6, 2, 0, 5, 0, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = nn.ReLU()\n",
    "R(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2abac-c7bc-4fa5-be73-120b081afb42",
   "metadata": {},
   "source": [
    "Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04494322-66b9-466f-9ec9-ffb9f7658663",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-3,3,100)\n",
    "y = R(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e00df6-1678-45c3-ad03-8c6db2ae7404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAes0lEQVR4nO3deXxU9dXH8c8BwxoWZYkiS1BARMsWBFzaEpcW0Yq1anGrWi2KuGvdi0sfba2t1SrV+lTrAhJQ1FLFuoKIrQoJe9iRHdm3EAJZzvNHxqcpBjJMZnJnbr7v12te3Jm7zDlMcuaX39w519wdERFJfXWCDkBEROJDBV1EJCRU0EVEQkIFXUQkJFTQRURC4pCgnrhly5aemZkZ0767du2icePG8Q0oIMolOYUll7DkAcrlG7m5uZvcvVVl6wIr6JmZmUyfPj2mfSdPnsyAAQPiG1BAlEtyCksuYckDlMs3zGzF/tZpykVEJCRU0EVEQkIFXUQkJFTQRURCQgVdRCQkqizoZtbAzL40s1lmNs/MHqxkm/pmNtbMlpjZF2aWmZBoRURkv6IZoe8BTnX3HkBPYKCZ9d9nm6uAre7eCfgj8GhcoxQRkSpVWdC9XEHkblrktm/P3cHAS5Hl14HTzMziFqWISEg8+eFiVuwoTcixLZp+6GZWF8gFOgEj3f3OfdbPBQa6++rI/aVAP3fftM92Q4GhABkZGVk5OTkxBV1QUEB6enpM+yYb5ZKcwpJLWPKAcOTy2Zpi/nfOXn7YzrnouNhyyc7OznX3PpWudPeob0BzYBJw/D6PzwXaVri/FGh5oGNlZWV5rCZNmhTzvslGuSSnsOQSljzcUz+X+eu2+zH3TfQhf/m3f/jRxzEfB5ju+6mrB3WWi7tvixT0gfusWgO0AzCzQ4BmwOaDObaISFjtLCpm2Kg8mjZI408X9aJuncTMSEdzlksrM2seWW4InAEs2GezCcDlkeXzgY8j7yQiIrWau3PH67NZuaWQpy/uTasm9RP2XNE05zoCeCkyj14HGOfub5vZQ5QP/ScAzwOvmNkSYAswJGERi4ikkOenfsW7c7/m3kHH0rfjYQl9rioLurvPBnpV8viICstFwAXxDU1EJLVNW76F37y7gIHHHc7V3+2Y8OfTN0VFRBJg4849DB+dR7tDG/K7C7pTE2dyB9YPXUQkrEpKy7hhTB7bdxfz4pV9adogrUaeVwVdRCTO/vDBIj5ftoXHzu9OtzZNa+x5NeUiIhJHH+Sv55nJSxlyQjsu6NOuRp9bBV1EJE5Wbi7k1nEzOf7IpjxwznE1/vwq6CIicVBUXMqw0bkY8MwlWTRIq1vjMWgOXUQkDh6YMI95a3fw/OV9aHdYo0Bi0AhdRKSaXpu+ipxpqxiefTSnHZsRWBwq6CIi1ZC/dgf3vTWXE49qwS2ndwk0FhV0EZEY7Sgq5rrRuTRrWN5065C6wZZUzaGLiMTA3bl93CxWbd1NztD+CW26FS2N0EVEYvC/ny7j/fz13H1mV07ITGzTrWipoIuIHKQvlm3m0X8uZNB3DueqUxLfdCtaKugiIgdhw84irh8zgw6HNeLRn9RM061oaQ5dRCRKJaVl3PDqDHYWFfPKVX1pUkNNt6Klgi4iEqXfv7+IL77awuMX9qDr4TXXdCtamnIREYnCB/nrefaTpVzcrz3n9W4bdDiVUkEXEanCis27uHXcTL5zZDNGnN0t6HD2SwVdROQAiopLuXZUHnXM+PMlvQNpuhUtzaGLiBzAiL/PZf66HbxwRXBNt6KlEbqIyH6Mm7aKcdNXc312J07tGlzTrWipoIuIVGLe2u386u9zOblTC245I9imW9FSQRcR2cf23cUMG5VH80ZpPDmkF3XrJM+Xhw5Ec+giIhWUlTm3jZvF2m27GXtNf1qmB990K1pVjtDNrJ2ZTTKzfDObZ2Y3VbLNADPbbmYzI7cRiQlXRCSx/jJlGR/OX889g44lq0NyNN2KVjQj9BLgNnfPM7MmQK6ZfeDu+fts96m7nx3/EEVEasa/l27msfcWcFb3I7jy5MygwzloVY7Q3X2du+dFlncC84EjEx2YiEhN2rCjiBvGzCCzZeOka7oVLXP36Dc2ywSmAMe7+44Kjw8AxgOrgbXA7e4+r5L9hwJDATIyMrJycnJiCrqgoID09PSY9k02yiU5hSWXsOQBic2lpMz53bQilu8o4/7+DTmySWLPF6lOLtnZ2bnu3qfSle4e1Q1IB3KB8ypZ1xRIjywPAhZXdbysrCyP1aRJk2LeN9kol+QUllzCkod7YnN55J1873Dn2/5m3uqEPUdF1ckFmO77qatRvQ2ZWRrlI/DR7v5GJW8KO9y9ILI8EUgzs5YH+cYjIlLj3pv3NX+ZsoxL+7fn3F6pPZsczVkuBjwPzHf3x/ezzeGR7TCzvpHjbo5noCIi8bZ80y5uHzeLHm2b8askbroVrWjOcjkZuAyYY2YzI4/dA7QHcPdngfOBYWZWAuwGhkT+NBARSUq795Zy7ahc6tY1Rl7Sm/qHJG/TrWhVWdDdfSpwwI973f1p4Ol4BSUikkjuzq/+PpeF63fytytOoO2hyd10K1r66r+I1Dpjp63i9dzV3HBqZwYc0zrocOJGBV1EapW5a7YzYsI8vtu5JTed1jnocOJKBV1Eao3thcUMG51Li8b1UqrpVrTUnEtEaoWyMue212by9fYixl5zIoc1rhd0SHGnEbqI1ArPTlnKh/M3cO+gY+nd/tCgw0kIFXQRCb1/Ld3E799byNndj+DykzKDDidhVNBFJNTW7yjixjEz6JjCTbeipTl0EQmt4tIyrn81j8K9pYz5RX8a1w93yQt3diJSqz367gKmLd/Kk0N60jmjSdDhJJymXEQklN6ds46/Tv2Kn53YgcE9U7vpVrRU0EUkdJZtLOCXr8+mR7vm3HvWsUGHU2NU0EUkVAr3ljBsVB5pdY0/h6TpVrQ0hy4ioeHu3PfmXBZt2MlLV/blyOYNgw6pRmmELiKh8eqXK3ljxhpuOq0z3+vSKuhwapwKuoiEwuzV23hwQj7f69KKG08NV9OtaKmgi0jK21a4l2Gj8miZXo8nftqTOiFruhUtzaGLSEorK3NuHTeLDTuLGBfSplvR0ghdRFLanycv4eMFG7jvrG70CmnTrWipoItIyvpsySYe/2AR5/Row89O7BB0OIFTQReRlPT19vKmW0e1Suc3530n1E23oqU5dBFJOcWlZQx/NY/dxaWMvbR36JtuRUv/CyKScn4zcQG5K7byp4t60al1+JtuRUtTLiKSUt6ZvY4XPvuKK07K5JwebYIOJ6mooItIyli6sYA7Xp9Fr/bNuWdQ7Wm6Fa0qC7qZtTOzSWaWb2bzzOymSrYxM/uTmS0xs9lm1jsx4YpIbbWnxBk2Kpf6aXUZeXFv6h2i8ei+oplDLwFuc/c8M2sC5JrZB+6eX2GbM4HOkVs/4JnIvyIi1ebuvDhvD4s3lPLyz/vSppY13YpWlW9x7r7O3fMiyzuB+cC+3eIHAy97uc+B5mZ2RNyjFZFaadQXK/n3ulJuOb0L3+1c+5puRcvcPfqNzTKBKcDx7r6jwuNvA79196mR+x8Bd7r79H32HwoMBcjIyMjKycmJKeiCggLS09Nj2jfZKJfkFJZcwpDHsu2lPPJ5EV2aO7f3bUydEJxvXp3XJTs7O9fd+1S2LurTFs0sHRgP3FyxmB8Md38OeA6gT58+PmDAgFgOw+TJk4l132SjXJJTWHJJ9Ty27trLvU9NJaNZQ67rZZyanR10SHGRqNclqk8VzCyN8mI+2t3fqGSTNUC7CvfbRh4TEYlJWZlzy7iZbNhZxMhLepNeL/VH5okWzVkuBjwPzHf3x/ez2QTgZ5GzXfoD2919XRzjFJFa5ulJS5i8cCMjzu5Gz3bNgw4nJUQz5XIycBkwx8xmRh67B2gP4O7PAhOBQcASoBC4Mu6Rikit8enijfzxw0Wc27MNl/ZX061oVVnQIx90HvBvHS//ZHV4vIISkdpr3fbd3JQzk86t03lETbcOis7MF5GksbekjOtG57GnuJQ/X5JFo3pqN3Uw9L8lIknjkYnzmbFyGyMv7k2n1ql9umUQNEIXkaTwj1lrefFfy/n5yR05q7u+lxgLFXQRCdySDQXcNX42WR0O5e5BXYMOJ2WpoItIoHbtKWHYqFwaRJpupdVVWYqV5tBFJDDuzt1vzGHpxgJeuaofhzdrEHRIKU1vhSISmFc+X8GEWWu59YwunNypZdDhpDwVdBEJxMxV2/j12/mc2rU11w3oFHQ4oaCCLiI1bsuuvVw3KpfWTRrw+IU9qFNHXx6KB82hi0iNKi1zbh47k00Fe3l92Ik0b1Qv6JBCQyN0EalRT328mCmLNnL/Od3o3rZ50OGEigq6iNSYKYs28uRHizmv95Fc3Ld90OGEjgq6iNSINdt2c1PODLq0bsLD56rpViKooItIwu0tKWP46DyKS51nLu1Nw3p1gw4plPShqIgk3MPv5DNz1TaevbQ3R7VS061E0QhdRBJqwqy1vPTvFVx9SkcGHq+mW4mkgi4iCbN4/U7uGj+bPh0O5c4z1XQr0VTQRSQhCvaUcO2oXBrVq8vIS9R0qyZoDl1E4s7duWv8bL7atItRV/cjo6mabtUEvWWKSNy99K/lvD17Hbf94BhOOlpNt2qKCrqIxFXeyq08PHE+p3VtzbDvHx10OLWKCrqIxM3mgj0MH53H4c0a8PiFPdV0q4ZpDl1E4uKbplubd+3ljWEn0axRWtAh1ToaoYtIXDz50WI+XbyJB885juOPbBZ0OLVSlQXdzF4wsw1mNnc/6weY2XYzmxm5jYh/mCKSzCYv3MBTHy/mJ73bMuSEdkGHU2tFM+XyIvA08PIBtvnU3c+OS0QiklJWby3k5rEzOSajCf9z7vFquhWgKkfo7j4F2FIDsYhIitlTUsrw0XmUljrPXpqlplsBM3eveiOzTOBtdz++knUDgPHAamAtcLu7z9vPcYYCQwEyMjKycnJyYgq6oKCA9PRwNPhRLskpLLkkOo+X8/fw8coSbuhVn6yMxJ5jEZbXBKqXS3Z2dq6796l0pbtXeQMygbn7WdcUSI8sDwIWR3PMrKwsj9WkSZNi3jfZKJfkFJZcEpnHWzNWe4c73/aH38lP2HNUFJbXxL16uQDTfT91tdpnubj7DncviCxPBNLMTF8NEwmxRet3ctf4OfTNPIw7fnhM0OFIRLULupkdbpFPQcysb+SYm6t7XBFJTt803Wpc/xCevrgXh6jpVtKoctLLzMYAA4CWZrYauB9IA3D3Z4HzgWFmVgLsBoZE/iwQkZBxd+4cP5sVmwsZfXU/WqvpVlKpsqC7+0VVrH+a8tMaRSTk/vbZct6ZvY67zuxK/6NaBB2O7EN/K4lIVHJXbOGRifM5o1sG13zvqKDDkUqooItIlcqbbs2gTfOG/P6CHvryUJJScy4ROaDSMuemnJlsKYw03WqoplvJSiN0ETmgJz5cxNQlm/j1YDXdSnYq6CKyX5MWbOCpj5dwYZ+2/PSE9kGHI1VQQReRSq3aUt50q9sRTXlo8Le6fkgSUkEXkW/ZU1LK8FfzKHPnmUt70yBNTbdSgT4UFZFveegf+cxevZ3nLsuiQ4vGQYcjUdIIXUT+y5szVjP6i5Vc8/2j+MFxhwcdjhwEFXQR+X8Lv97J3W/MoV/Hw/jlD9R0K9WooIsIADuLihk2KpcmDdJ4Sk23UpLm0EUEd+eO12ezYkshr17dj9ZN1HQrFektWER4fupXvDv3a+744TH0U9OtlKWCLlLLTV++hd++u4AfdMtgqJpupTQVdJFabFPBHoa/mseRhzbkMTXdSnmaQxeppUrLnBvHzGBbYTFvXtdXTbdCQAVdpJZ6/IOF/GvpZn53fne6tWkadDgSB5pyEamFPpq/npGTljLkhHZc2Kdd0OFInKigi9Qyq7YUcsvYmRzXpikPnHNc0OFIHKmgi9QiRcWlDBudiwPPXJKlplshozl0kVrkwX/kM3fNDv76sz60b9Eo6HAkzjRCF6klxueuZsyXKxk24GhO75YRdDiSACroIrXAgq93cO9bczjxqBbcdkaXoMORBFFBFwm5HUXFDBuVR9MGafzpIjXdCrMqX1kze8HMNpjZ3P2sNzP7k5ktMbPZZtY7/mGKSCzcnTtem83KLYU8fXFvWjWpH3RIkkDRvFW/CAw8wPozgc6R21DgmeqHJSLx8M/lJfxz3tfcNbArfTseFnQ4kmBVFnR3nwJsOcAmg4GXvdznQHMzOyJeAYpIbKYt38Jri/Yy8LjDufq7HYMOR2qAuXvVG5llAm+7+7cu/W1mbwO/dfepkfsfAXe6+/RKth1K+SiejIyMrJycnJiCLigoID09PaZ9k41ySU6pnsu2PWU88K8i0qyMB09uTKO01G+6leqvSUXVySU7OzvX3ftUtq5Gz0N39+eA5wD69OnjAwYMiOk4kydPJtZ9k41ySU6pnEtJaRmXPv8FRWV7uK1vIwadkR10SHGRyq/JvhKVSzw+7l4DVGwG0TbymIgE4A8fLOLzZVv4n3O/Q7smOqOlNonHqz0B+FnkbJf+wHZ3XxeH44rIQfowfz3PTF7KRX3bc35W26DDkRpW5ZSLmY0BBgAtzWw1cD+QBuDuzwITgUHAEqAQuDJRwYrI/q3cXMgt42Zy/JFNuf9H3YIORwJQZUF394uqWO/A8LhFJCIH7ZumW3XM1HSrFlNzLpEQeGDCPOat3cELV/Sh3WFqulVb6RMTkRT32vRV5ExbxfDsozm1q5pu1WYq6CIpLH/tDu57ay4nHd2CW884JuhwJGAq6CIpavvuYoaNzqV5o/KmW3XrpP6Xh6R6NIcukoLcnV++Nos1W3eTM7Q/LdPVdEs0QhdJSc9NWcb7+eu568yu9MlU0y0pp4IukmK+WLaZ3723kDOPP5yrTlHTLfkPFXSRFLJhRxHXj5lBh8Ma8bvzu2OmeXP5D82hi6SIktIyrh8zg51FxbxyVV+aNEgLOiRJMiroIinisfcX8uVXW3j8wh50Pbxp0OFIEtKUi0gKeG/e1/zlk2Vc3K895/VW0y2pnAq6SJJbvmkXt4+bRfe2zdR0Sw5IBV0kiZU33cqjTh1j5MW9qX+Imm7J/mkOXSSJ/eqtucxft4O/XXGCmm5JlTRCF0lSY6et5LXc1dxwaieyu7YOOhxJASroIklo7prt/Orv8zilU0tuPr1L0OFIilBBF0ky2wuLuW50Hoc1qseTQ3qq6ZZETXPoIkmkrMy57bWZrN22m7HXnEgLNd2Sg6ARukgSeXbKUj6cv4F7Bh1LVodDgw5HUowKukiS+PfSzfz+vYWc1f0Irjw5M+hwJAWpoIskgQ07irhhzAwyWzbm0Z+o6ZbERnPoIgErLi3j+ldnsGtPCa/+oh/p9fVrKbHRT45IwB57byFfLt/CEz/tSZeMJkGHIylMUy4iAfrn3HU8N2UZl/XvwLm9jgw6HElxURV0MxtoZgvNbImZ3VXJ+ivMbKOZzYzcro5/qCLh8tWmXfzytdn0aNec+84+NuhwJASqnHIxs7rASOAMYDUwzcwmuHv+PpuOdffrExCjSOjs3lvKsFG51K1rjLy4l5puSVxEM0LvCyxx92XuvhfIAQYnNiyR8HJ37ntrLgvX7+SJn/ak7aFquiXxYe5+4A3MzgcGuvvVkfuXAf0qjsbN7ArgN8BGYBFwi7uvquRYQ4GhABkZGVk5OTkxBV1QUEB6enpM+yYb5ZKcEpnL5FXFvDhvL4OPTuPHnesl5Dm+odckOVUnl+zs7Fx371PpSnc/4A04H/hrhfuXAU/vs00LoH5k+Rrg46qOm5WV5bGaNGlSzPsmG+WSnBKVy5zV27zzvRP90r9+7iWlZQl5jor0miSn6uQCTPf91NVoplzWAO0q3G8beazim8Jmd98TuftXICu69xqR2mN7YTHDRufSonE9nhzSS023JO6iKejTgM5m1tHM6gFDgAkVNzCzIyrcPQeYH78QRVJfWZlzy7iZfL29iJGX9OawxomdapHaqcqzXNy9xMyuB94D6gIvuPs8M3uI8qH/BOBGMzsHKAG2AFckMGaRlPPMJ0v5eMEGHvhRN3q3V9MtSYyovinq7hOBifs8NqLC8t3A3fENTSQcPluyiT+8v5Af9WjD5SdlBh2OhJi+KSqSQF9vL+KmnBl0bNmY3573HTXdkoRSLxeRBClvupVH4d5SxvyiP43VdEsSTD9hIgny23cXMH3FVp4c0pPOarolNUBTLiIJMHHOOp6f+hWXn9iBwT3VdEtqhgq6SJwt21jAHa/Ppme75tx7Vregw5FaRAVdJI4K95YwbFQeaXWNP1/Sm3qH6FdMao7m0EXixN257825LNqwk5d/3pc2zRsGHZLUMho+iMTJq1+u5I0Za7j5tC58t3OroMORWkgFXSQOZq/exoMT8vl+l1bccGqnoMORWkoFXaSathXuZdioPFo1qc8TP+1JHTXdkoBoDl2kGsrKnJvHzmTDziJeu/YkDlXTLQmQRugi1TBy0hImL9zIiLO70bNd86DDkVpOBV0kRlMXb+LxDxcxuGcbLu3fIehwRFTQRWKxbvtubsyZQadW6TzyYzXdkuSggi5ykPaWlDF8dB57ikt55tIsNd2SpKGfRJGD9Jt355O3chtPX9yLTq3DcdFiCQeN0EUOwjuz1/G3z5ZzxUmZnN29TdDhiPwXFXSRKC3ZUMAdr8+id/vm3DPo2KDDEfkWFXSRKBTuLeG60bnUT6vLSDXdkiSlOXSRKrg797wxh8UbCnjl5/04opmabkly0jBDpAqjvljJWzPXcuvpXTilc8ugwxHZLxV0kQOYtWobv/5HPgOOacXwbDXdkuSmgi6yH1t37eW60Wq6JalDc+gilSjz8qZbG3fu4fVhJ9K8kZpuSfKLaoRuZgPNbKGZLTGzuypZX9/MxkbWf2FmmXGPVKSG7NpTwovz9vLJoo2M+FE3urdtHnRIIlGpsqCbWV1gJHAm0A24yMz2vfLtVcBWd+8E/BF4NN6BitSETxdv5IdPTGHK6hKu+f5RXNKvfdAhiUQtmimXvsASd18GYGY5wGAgv8I2g4EHIsuvA0+bmbm7xzFWAD5ZtJF7phbSOO+TeB86ELsKlUuyKHVn2cZdHNWyMff0a8DQM/XlIUkt0RT0I4FVFe6vBvrtbxt3LzGz7UALYFPFjcxsKDAUICMjg8mTJx90wEu2lpJRv4y6tvug901G6coleRj06JTGmR2dvbt3x/TzmWwKCgpCkQcol2jU6Iei7v4c8BxAnz59fMCAAQd9jAFAp8mTiWXfZDRZuSSlsOQSljxAuUQjmg9F1wDtKtxvG3ms0m3M7BCgGbA5HgGKiEh0oino04DOZtbRzOoBQ4AJ+2wzAbg8snw+8HEi5s9FRGT/qpxyicyJXw+8B9QFXnD3eWb2EDDd3ScAzwOvmNkSYAvlRV9ERGpQVHPo7j4RmLjPYyMqLBcBF8Q3NBERORj66r+ISEiooIuIhIQKuohISKigi4iEhAV1dqGZbQRWxLh7S/b5FmoKUy7JKSy5hCUPUC7f6ODurSpbEVhBrw4zm+7ufYKOIx6US3IKSy5hyQOUSzQ05SIiEhIq6CIiIZGqBf25oAOII+WSnMKSS1jyAOVSpZScQxcRkW9L1RG6iIjsQwVdRCQkUragm9mvzWy2mc00s/fNrE3QMcXKzB4zswWRfN40s+ZBxxQrM7vAzOaZWZmZpdwpZlVdED1VmNkLZrbBzOYGHUt1mVk7M5tkZvmRn62bgo4pFmbWwMy+NLNZkTwejPtzpOocupk1dfcdkeUbgW7ufm3AYcXEzH5AeQ/5EjN7FMDd7ww4rJiY2bFAGfAX4HZ3nx5wSFGLXBB9EXAG5ZdanAZc5O75B9wxCZnZ94AC4GV3Pz7oeKrDzI4AjnD3PDNrAuQC56ba62JmBjR29wIzSwOmAje5++fxeo6UHaF/U8wjGgOp+c4EuPv77l4Sufs55VeFSknuPt/dFwYdR4z+/4Lo7r4X+OaC6CnH3adQfm2ClOfu69w9L7K8E5hP+XWMU4qXK4jcTYvc4lq3UragA5jZw2a2CrgEGFHV9ini58C7QQdRS1V2QfSUKxxhZmaZQC/gi4BDiYmZ1TWzmcAG4AN3j2seSV3QzexDM5tbyW0wgLvf6+7tgNHA9cFGe2BV5RLZ5l6ghPJ8klY0uYjEm5mlA+OBm/f5Cz1luHupu/ek/K/wvmYW1+mwqK5YFBR3Pz3KTUdTfkWl+xMYTrVUlYuZXQGcDZyW7NdjPYjXJdVEc0F0CUBkznk8MNrd3wg6nupy921mNgkYCMTtg+ukHqEfiJl1rnB3MLAgqFiqy8wGAncA57h7YdDx1GLRXBBdaljkw8Tngfnu/njQ8cTKzFp9cwabmTWk/MP3uNatVD7LZTxwDOVnVKwArnX3lBxNRS6uXR/YHHno8xQ+Y+fHwFNAK2AbMNPdfxhoUAfBzAYBT/CfC6I/HGxEsTGzMcAAytu0rgfud/fnAw0qRmZ2CvApMIfy33eAeyLXOk4ZZtYdeInyn606wDh3fyiuz5GqBV1ERP5byk65iIjIf1NBFxEJCRV0EZGQUEEXEQkJFXQRkZBQQRcRCQkVdBGRkPg/OCQTZVf0qNQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x.numpy(), y.numpy())\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ebbed-9bac-4f83-866f-0c1134729b44",
   "metadata": {},
   "source": [
    "How much better does our model do with this simple adjustment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2e1232d-ecc6-49c5-b827-8715a63c6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,8,bias=False)\n",
    "        self.Matrix2 = nn.Linear(8,1,bias=False)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2caf3-84fd-4c56-90d5-3cc1ac16d633",
   "metadata": {},
   "source": [
    "Train model (lets write a function to do this, since we'll be doing it a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c79d0074-a1b5-48a5-a061-761b5380f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x,y,f, n_epochs=50):\n",
    "    opt = SGD(f.parameters(), lr=0.001)\n",
    "    L = nn.MSELoss()\n",
    "\n",
    "    # Train model\n",
    "    losses = []\n",
    "    for _ in range(n_epochs):\n",
    "        opt.zero_grad() # flush previous epoch's gradient\n",
    "        loss_value = L(f(x), y) #compute loss\n",
    "        loss_value.backward() # compute gradient\n",
    "        opt.step() # Perform iteration using gradient above\n",
    "        losses.append(loss_value.item())\n",
    "    return f, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec199b6-e8a5-41b0-b79e-5b57e4cf2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "f2 = MyNeuralNet2()\n",
    "\n",
    "# Train model\n",
    "f2, losses2 = train_model(x,y,f2, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136eefb-8135-4cca-b11d-b54230b7219c",
   "metadata": {},
   "source": [
    "Now lets look at the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bb25296-9055-4015-8c4c-fa703f876ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41ef4276-0202-4db0-a175-a6e3890ff3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2595, 2.8164, 1.3443, 4.9042], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2ccf9-5d48-4397-89de-694e8ec9af76",
   "metadata": {},
   "source": [
    "Slightly better. But the real advantage of this slight non-linearity is that we can make our matrices much larger. Lets make our matrices size $80 \\times 2$ and $1 \\times 80$. This **only** works because of our non-linearity function $R(x)$:\n",
    "\n",
    "* Without $R(x)$, we would just have $A_2 A_1 = B$ and so $f(x) = Bx$ where $B$ is still a $1 \\times 2$ matrix even though $A_2$ and $A_1$ are larger matrices. The non-linearity function $R(x)$, to some extent, makes all 240 parameters more independent from eachother.\n",
    "\n",
    "$$\\text{Old Model}: \\hspace{5mm} f_2(x) = A_2 R(A_1 x) \\hspace{8mm} \\text{$A_2$ is 1x8 and $A_1$ is 8x2}$$\n",
    "\n",
    "$$\\text{New Model}: \\hspace{5mm} f_3(x) = A_2 R(A_1 x)  \\hspace{8mm} \\text{$A_2$ is 1x80 and $A_1$ is 80x2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6218c675-2b5e-480a-80c1-a015094cd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,80, bias=False)\n",
    "        self.Matrix2 = nn.Linear(80,1, bias=False)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3639332-e890-4154-a5c7-148b22195b7a",
   "metadata": {},
   "source": [
    "Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a08204-e82b-4a18-8148-098f0459fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "f3 = MyNeuralNet3()\n",
    "\n",
    "# Train model\n",
    "f3, losses3 = train_model(x,y,f3, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8507852-beff-4877-976b-340caea1b267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d683b12-cc1c-49f2-861f-fcf56d4cad47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8806, 3.7048, 1.8780, 5.1535], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5969e-a58f-4bbf-9570-7f76a566ddd7",
   "metadata": {},
   "source": [
    "Closer, but still not exact. We can make our model better by introducing other parameters:\n",
    "\n",
    "$$f(x) = A_2 R(A_1x + b_1) + b_2$$\n",
    "\n",
    "where $b_1$ and $b_2$ are vectors added to each of the linear transformations.\n",
    "\n",
    "$$\\text{Old Model}: \\hspace{5mm} f_3(x) = A_2 R(A_1 x) \\hspace{8mm} \\text{$A_2$ is 1x80 and $A_1$ is 80x2}$$\n",
    "\n",
    "$$\\text{New Model}: \\hspace{5mm} f_4(x) = A_2 R(A_1 x+b_1)+b_2  \\hspace{8mm} \\text{$A_2$ is 1x80 and $A_1$ is 80x2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eb086c4-eedc-47da-8bef-56e5e7f9fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,80)\n",
    "        self.Matrix2 = nn.Linear(80,1)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c79a77b9-3b43-43b7-b37c-8af541d55018",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "f4 = MyNeuralNet4()\n",
    "\n",
    "# Train model\n",
    "f4, losses4 = train_model(x,y,f4, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eb93aa7-2c6e-4b67-befa-8472246d4329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c75560f6-fbc5-4f35-b8af-d28370adaa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4893, 4.3751, 2.0141, 5.0303], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa448b68-7e32-4ffb-85c9-9dceba98676c",
   "metadata": {},
   "source": [
    "Better, but its still not getting us that close to $y$, however. What if we add another matrix in the middle?\n",
    "\n",
    "$$\\text{Old Model}: \\hspace{5mm} f_4(x) = A_2 R(A_1 x+b_1)+b_2  \\hspace{8mm} \\text{$A_2$ is 1x80 and $A_1$ is 80x2}$$\n",
    "\n",
    "$$\\text{New Model}: \\hspace{5mm} f_5(x) = A_3 R(A_2 R(A_1 x+b_1)+b_2)  \\hspace{8mm} \\text{$A_3$ is 1x80 and $A_2$ is 80x80 and $A_1$ is 80x2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec4a8bb3-96a5-4510-b50c-6af860270234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,80)\n",
    "        self.Matrix2 = nn.Linear(80,80)\n",
    "        self.Matrix3 = nn.Linear(80,1)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.R(self.Matrix2(x))\n",
    "        x = self.Matrix3(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()\n",
    "f5 = MyNeuralNet5()\n",
    "\n",
    "# Train model\n",
    "f5, losses5 = train_model(x,y,f5, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f68965af-a88f-4fd0-b391-286b183fd20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df7ea596-be20-4f53-826d-eaf41150498f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9995, 4.9961, 1.9994, 4.9981], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f5(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9932ef-d55f-4b2c-a933-bd72fb3e2d5b",
   "metadata": {},
   "source": [
    "Its predicting $y$ almost exactly (albeit by overfitting, no doubt, but the message here is that the model has the potential to fit to these arbitrary data points, through a sequence of **linear** transofmrations followed by slightly non-linear )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec711b12-f331-4150-89ca-b5b90456583e",
   "metadata": {},
   "source": [
    "# The \"Sequential\" Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e899fb5-8fec-48cc-a711-81da792324fb",
   "metadata": {},
   "source": [
    "A general \"sequential\" neural network can be expressed as\n",
    "\n",
    "$$f(x) = \\underset{i=1}{\\overset{n}{\\Huge{\\kappa}}} R_i(A_ix+b_i)$$\n",
    "\n",
    "where $\\underset{i=1}{\\overset{n}{\\Huge{\\kappa}}}f_i(x) = f_n \\circ f_{n-1} ... \\circ f_1(x)$ and the $A_i$ are matrices and the $b_i$ are bias vectors. Typically the $R_i$ are the same for all the layers (typically ReLU) **except** for the last layer, where $R_i$ is just is just the identity function\n",
    "\n",
    "* **Note**: In clever architectures, like convolutional neural networks, the $A_i$'s become sparse matrices (most of there parameters are fixed to equal zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317894c4-0fbb-4baf-abdf-b576cb9ed68a",
   "metadata": {},
   "source": [
    "# Next Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f8132-e249-4659-971b-c0c16c212215",
   "metadata": {},
   "source": [
    "Training on some real data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
